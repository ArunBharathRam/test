{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArunBharathRam/test/blob/master/Finalproject_braintumour_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so4ixbHCsGc2",
        "outputId": "c9508285-3937-48c6-eed3-9b6b6762a0c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M0plND33CE-",
        "outputId": "8cc70b1f-25ca-4f2d-b4e2-9b762133f736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['New document.exe', 'Scrap.shs', 'student affidavit.pdf', 'STATISTICS OF INDIAN CINEMA.pptx', 'Resume.gdoc', 'Group 21(review-2).pptx', 'ANNEXURE 2.docx.gdoc', 'aadhar front (1).jpg', 'New Doc 2017-10-10 (1).jpg', 'aadhar back (1).jpg', 'Char verification front.pdf', 'char form back_1.pdf', 'char form back_2.pdf', 'char form back_3.pdf', 'char form back_4.pdf', 'char form back_5.pdf', 'char form back_6.pdf', 'char form back_8.pdf', 'New Doc 2017-10-09.pdf', 'char form back_9.pdf', 'aadhar1.pdf', 'aadhar2.pdf', 'char front.pdf', 'char front_2.pdf', 'characterform2.pdf', 'adhar card1 - Page 1.pdf', 'adhar card1 - adhar card 2.pdf', 'edited', 'page 2 bgc excel.pdf', 'page 3 bgc (1).pdf', 'page 3 bgc.pdf', 'OCT_2017.pdf', 'SpiceJet_E-ticket_PNR K2K4RB - 28 Jan 2018 Madurai-Chennai for MR. KUMAR A S.PDF', 'SpiceJet_Mobile_PNR K2K4RB - 28 Jan 2018 Madurai-Chennai for MR. KUMAR A S.PNG', 'IMG_4618.JPG', 'IMG_20170729_145000.jpg', 'IMG_20170729_144930.jpg', 'IMG_20180204_222358_594.jpg', 'IMG_20171215_190647_394.jpg', 'IMG_20180126_223213_608.jpg', 'IMG_20171204_120935_730.jpg', 'IMG_20171014_154830_073.jpg', 'IMG_20170730_173529_144.jpg', 'Mküòéüòçhttps:  media.giphy.com media 5GoVLqeAOohttps:  media.giphy.com media 5GoVLqeAOo6PK giphy.gif https:  media.giphy.com media 5GoVLqeAOo6PK giphy.gif üéÅk7m, you mmmz. I? I 6PK giphy.gif .gdoc', 'Untitled.gdoc', 'Classroom', 'Untitled form.gform', 'Selva Kumar A S(18138)-resume.pdf', 'Internship Resume-Selva Kumar(18138).pdf', 'Resume-Selva Kumar A S-FINAL.pdf', 'Selva Kumar A S(18138)-last edited resume.pdf', \"Share 'C_18138_Selva Kumar.A.S.pptx'\", \"Share 'Assignment 1_CB.BU.P2MBA18138.docx'\", 'Latest-Updated-Resume-Selva Kumar A S (1).pdf', 'Latest-Updated-Resume-Selva Kumar A S.pdf', 'BusComm T3 2019 TEAMS.xlsx', 'Scanned phot copy.pdf', 'Request letter bank.pdf', 'Listen to me sing \"poovukkul olinthirukkum short version üÖΩüÖ¥üÜÜ\" on Smule', 'Selva Kumar A S _Section _C_CB.BU.P2MBA18138.gdoc', 'MVBE Teaching notes Revised 2019.docx', 'To-do list.gsheet', 'Untitled spreadsheet (1).gsheet', '1366426_RL.pdf', 'MAY_2018.pdf', 'Offer letter.pdf', 'intern certificate.pdf', '177057.pdf', 'Selva Kumar-CB.BU.P2MBA18138.docx', 'Selva Kumar-CB.BU.P2MBA18138KASH.docx.pptx', 'CIR Resume.pdf', 'CIR_Resume_Selva Kumar.pdf', 'CIR Resume-Selva Kumar 1 (2).pdf', '1566559622929_CIR Resume-Selva Kumar 1.pdf', 'CIR Resume-Selva Kumar 1 (1).pdf', 'Campus Process - CP Final Round.pdf', 'CIR Resume-Selva Kumar 1.pdf', 'Untitled presentation.gslides', 'Social Media Peer influence on Consumer Behaviour.pdf', 'Social Media Peer influence on Consumer Behaviour.gdoc', 'Selva resume.pdf', 'Selva resume.gdoc', 'Selva Kumar_18138_MCCM.docx', 'Untitled Diagram.drawio', 'Impact of discount rates on online hotel selection criteria among the customers  (Responses).gsheet', 'CIR Resume-Selva Kumar 1 new.pdf', 'aadhar back.jpg', 'aadhar front.jpg', 'orchids the international schl kolshet thane mr.gsheet', 'new horizon pub sch navi mumbai mr.gsheet', 'new horizon scholars school airoli navimumbai mr.gsheet', 'Sep 21 Full  time and part time Positive Leads.gsheet', 'CIR Resume-Selva Kumar updated (1).pdf', 'CIR Resume-Selva Kumar updated.pdf', 'IMG-20210109-WA0003.jpeg', 'Selvakumar - Offer Letter.pdf', 'Selvakumar - Offer Letter.gdoc', 'New', '1366426_SC - selva kumar.pdf', 'Latest-Updated-Resume-Selva Kumar A S - selva kumar - selva kumar.pdf', 'Latest-Updated-Resume-Selva Kumar A S - selva kumar.pdf', 'Selva Kumar A S(18138)-last edited resume - selva kumar.pdf', 'MAY_2018 - selva kumar.pdf', 'Internship Resume-Selva Kumar(18138) - selva kumar.pdf', 'Resume-Selva Kumar A S-FINAL - selva kumar.pdf', 'Offer letter - selva kumar.pdf', '1366426_RL - selva kumar.pdf', '177057 - selva kumar.pdf', 'Selva Kumar A S - selva kumar.pdf', 'Selva Kumar 2022 resume.pdf', 'Growfin Activity Selva Kumar A S.gdoc', 'Untitled document (2).gdoc', 'Document from Selva Ramesh Kumar', 'Untitled document (1).gdoc', 'TCS Service certificate.pdf', 'Selva Kumar 2022 resume.gdoc', 'HZKPS9879F_PARTB_2023-24.pdf', 'Rc book back_1.jpg', 'Rc book back (1).pdf', 'Rc book front.pdf', 'Driving license  front.pdf', 'Rc book back.pdf', 'Driving license back.pdf', 'pan card (1) (1).pdf', 'Relieving Letter - Resigned Employees_Selva Kumar A S.pdf', 'Copy of Relieving Letter - Resigned Employees_Selva Kumar A S.pdf', 'Vaccination certificate.jpg', 'pwds', 'IT return filing 23 - 24.pdf', 'IT return filing 23 to 24 extended.pdf', 'passport back.pdf', 'passport front.pdf', 'E signature.pdf', 'singapore album.mp4', 'Singapore 2', 'Selvakumar_Resume_4YoE (1).pdf', 'PreMagic content.gdoc', 'Selvakumar_Resume_2024_latest.pdf', 'NSDC reg screenshot.png', 'May 2024 bank statement.pdf', 'image (1) (1).png', 'image (1).png', 'Colab Notebooks', 'codekata1.png', 'MBA transcript.pdf', 'Grade sheet year 1.pdf', 'Provisional  certificate MBA.pdf', 'Zuddl intro.mp4', 'DOC-20231026-WA0000..pdf', 'DOC-20231026-WA0000..gdoc', 'Aishwaaryaa Updated Resume.gdoc', 'Cover letter SDR.pdf', 'Maino.ai script.gdoc', 'Cover letter latest.gdoc', 'SelvakumarASResume (1).docx', 'Hubapot intro.mp4', 'SelvakumarASResume (1) (2).pdf', 'SelvakumarASResume (1) (1).pdf', 'photo_new.jpg', 'WES aishu UG.pdf', 'WES aishu PG.pdf', 'Untitled document.gdoc', 'Cover letter Selvakumar 2024 (1).pdf', 'SelvakumarASResume (1).pdf', 'Selvakumar Assignment Stitchflow .gdoc', 'Untitled spreadsheet.gsheet', 'Rategain cold call script.gdoc', 'Copy of Cover letter latest.pdf', 'Selenium.png', 'data1', '.ipynb_checkpoints', 'sample_data', 'sample_data1', 'Aishu Ug degree certificate.pdf', 'Screenshot 2025-03-07 154757.png', 'aadhar new front (1) (1)_page-0001.jpg', 'pan card (1)_page-0001.jpg', 'PG degree certificate_page-0001.jpg', 'brain_tumor_dataset (3)', 'iLovePDF', 'Aishu Aadhar card.pdf', 'Candle light dinner ', 'VN20251104_201049.mp4', 'Scuba andaman.mp4', 'Kundrathur rental agreement.pdf', 'car_dekho_project', 'all_cities_cars_combined.csv', 'Final_project_datascience']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.listdir('/content/drive/MyDrive'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DouEManu40EF",
        "outputId": "921b976c-3bd5-4884-e49c-47531a4510bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Testing', 'Training', 'Validation']\n"
          ]
        }
      ],
      "source": [
        "dataset_path = \"/content/drive/MyDrive/brain_tumor_dataset (3)\"\n",
        "print(os.listdir(dataset_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xj0VaVT646i7"
      },
      "outputs": [],
      "source": [
        "train_dir = \"/content/drive/MyDrive/Training\"\n",
        "test_dir = \"/content/drive/MyDrive/Testing\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kWRbl-Q48lL",
        "outputId": "9acf3530-31cd-4929-c2b1-77aa5680eb71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "FOUND HERE: /content/drive/MyDrive/brain_tumor_dataset (3)\n",
            "FOUND HERE: /content/drive/MyDrive/Final_project_datascience/brain_tumor_dataset\n",
            "FOUND HERE: /content/drive/.Encrypted/MyDrive/brain_tumor_dataset (3)\n",
            "FOUND HERE: /content/drive/.Encrypted/MyDrive/Final_project_datascience/brain_tumor_dataset\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "# Search entire MyDrive for the folder names\n",
        "for root, dirs, files in os.walk('/content/drive', topdown=True):\n",
        "    if \"Training\" in dirs or \"Testing\" in dirs:\n",
        "        print(\"FOUND HERE:\", root)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKt3COTh5Rbn"
      },
      "outputs": [],
      "source": [
        "train_dir = \"/content/drive/MyDrive/brain_tumor_dataset (3)/Training\"\n",
        "test_dir  = \"/content/drive/MyDrive/brain_tumor_dataset (3)/Testing\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Lmq2bB65dfv",
        "outputId": "10b263d0-fe5d-491b-c24d-6b5202e7f8fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Option 1: ['Testing', 'Training', 'Validation']\n",
            "Option 2: ['Testing', 'Training']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(\"Option 1:\", os.listdir(\"/content/drive/MyDrive/brain_tumor_dataset (3)\"))\n",
        "print(\"Option 2:\", os.listdir(\"/content/drive/MyDrive/Final_project_datascience/brain_tumor_dataset\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"/content/drive/MyDrive/Final_project_datascience/brain_tumor_dataset/Training\"\n",
        "val_dir   = \"/content/drive/MyDrive/Final_project_datascience/brain_tumor_dataset/Validation\"\n",
        "test_dir  = \"/content/drive/MyDrive/Final_project_datascience/brain_tumor_dataset/Testing\"\n"
      ],
      "metadata": {
        "id": "8pQ7jjBrYhQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"/content/drive/MyDrive/Final_project_datascience/brain_tumor_dataset/Training\"\n",
        "test_dir  = \"/content/drive/MyDrive/Final_project_datascience/brain_tumor_dataset/Testing\"\n"
      ],
      "metadata": {
        "id": "HvFdd07zHSX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "metadata": {
        "id": "9AnHKQ1yiu6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")\n",
        "print(\"Using CPU only\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLxos0oYizER",
        "outputId": "54085c5d-969c-43e4-974e-b25548d5698d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPU only\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Data Preprocessing & Augmentation\n",
        "IMG_SIZE = 128\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "LR = 0.0003\n",
        "\n"
      ],
      "metadata": {
        "id": "2HbbwlMZi8eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transforms\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),           # Standard size for ResNet\n",
        "    transforms.RandomHorizontalFlip(),       # Augmentation\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "Ge6zGUQ1i_wC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jo5tziBizdG",
        "outputId": "368c7163-62c8-4bc9-e7c0-7a7bc46b07d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\n",
        "test_dataset  = datasets.ImageFolder(root=test_dir, transform=test_transform)\n",
        "\n",
        "# Compute class weights for imbalance\n",
        "class_counts = [0]*len(train_dataset.classes)\n",
        "for _, label in train_dataset.samples:\n",
        "    class_counts[label] += 1\n",
        "\n",
        "class_weights = 1.0 / torch.tensor(class_counts, dtype=torch.float)\n",
        "weights = [class_weights[label] for _, label in train_dataset.samples]\n",
        "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler, num_workers=2)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"Training images:\", len(train_dataset))\n",
        "print(\"Testing images:\", len(test_dataset))\n",
        "print(\"Classes:\", train_dataset.classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHhQZb3Oi4B-",
        "outputId": "02f83efd-331d-41f2-fc25-056547923466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images: 5693\n",
            "Testing images: 1311\n",
            "Classes: ['glioma', 'meningioma', 'notumor', 'pituitary']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# Load pretrained ResNet18\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the final fully connected layer\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))  # 4 classes\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLHV88hNjUqM",
        "outputId": "d6171056-cb22-485c-f56b-88d0c5b40c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((96,96)),      # Smaller = faster\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((96,96)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
        "])\n"
      ],
      "metadata": {
        "id": "uGWEw16VlY2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dir = \"/content/drive/MyDrive/Final_project_datascience/brain_tumor_dataset/Training\"\n",
        "test_dir = \"/content/drive/MyDrive/Final_project_datascience/brain_tumor_dataset/Testing\"\n",
        "\n",
        "train_dataset = ImageFolder(root=train_dir, transform=train_transform)\n",
        "test_dataset = ImageFolder(root=test_dir, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "yliZwCcOlcXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = resnet18(weights=None)  # Start fresh to avoid deprecated weights issues\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 4)  # 4 classes\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "Sl6ITlj2lf9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "L03Rsmknli-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(outputs.argmax(1) == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                preds = outputs.argmax(1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f}\")\n",
        "        print(classification_report(all_labels, all_preds, target_names=train_dataset.classes))\n"
      ],
      "metadata": {
        "id": "yGaHPGHxlmn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r# ===============================\n",
        "# Step 0: Imports & Setup\n",
        "# ===============================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "import os\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ===============================\n",
        "# Step 1: Paths\n",
        "# ===============================\n",
        "train_dir = \"/content/drive/MyDrive/Final_project_datascience/brain_tumor_dataset/Training\"\n",
        "test_dir  = \"/content/drive/MyDrive/Final_project_datascience/brain_tumor_dataset/Testing\"\n",
        "\n",
        "# ===============================\n",
        "# Step 2: Transforms\n",
        "# ===============================\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((96, 96)),  # smaller = faster\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((96, 96)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# ===============================\n",
        "# Step 3: Datasets & Loaders\n",
        "# ===============================\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "test_dataset  = datasets.ImageFolder(test_dir, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f\"Training images: {len(train_dataset)}\")\n",
        "print(f\"Testing images: {len(test_dataset)}\")\n",
        "print(\"Classes:\", train_dataset.classes)\n",
        "\n",
        "# ===============================\n",
        "# Step 4: Model (ResNet18)\n",
        "# ===============================\n",
        "model = models.resnet18(weights=None)  # start from scratch\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 4)      # 4 classes\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ===============================\n",
        "# Step 5: Training Loop\n",
        "# ===============================\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] | Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.4f}\")\n",
        "\n",
        "# ===============================\n",
        "# Step 6: Evaluation on Test Set\n",
        "# ===============================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=train_dataset.classes))\n",
        "\n",
        "# ===============================\n",
        "# Step 7: Save the Model\n",
        "# ===============================\n",
        "torch.save(model.state_dict(), \"brain_tumor_resnet18.pth\")\n",
        "print(\"Model saved as brain_tumor_resnet18.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QugKsbVjq4xk",
        "outputId": "9329d57f-3c89-4d6b-f906-7e678761eeef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Training images: 5693\n",
            "Testing images: 1311\n",
            "Classes: ['glioma', 'meningioma', 'notumor', 'pituitary']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5] | Loss: 0.5182 | Accuracy: 0.7948\n",
            "Epoch [2/5] | Loss: 0.2729 | Accuracy: 0.8978\n",
            "Epoch [3/5] | Loss: 0.1896 | Accuracy: 0.9359\n",
            "Epoch [4/5] | Loss: 0.1276 | Accuracy: 0.9547\n",
            "Epoch [5/5] | Loss: 0.1021 | Accuracy: 0.9665\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      glioma       0.97      0.89      0.93       300\n",
            "  meningioma       0.75      0.96      0.84       306\n",
            "     notumor       0.99      0.84      0.91       405\n",
            "   pituitary       0.99      0.99      0.99       300\n",
            "\n",
            "    accuracy                           0.91      1311\n",
            "   macro avg       0.92      0.92      0.92      1311\n",
            "weighted avg       0.93      0.91      0.92      1311\n",
            "\n",
            "Model saved as brain_tumor_resnet18.pth\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyM/9NWwgYPaKqXhUMsCaSpJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}